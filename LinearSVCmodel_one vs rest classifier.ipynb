{"cells":[{"metadata":{},"cell_type":"markdown","source":"Narges Ghasemi 97243050 "},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install https://github.com/sobhe/hazm/archive/master.zip --upgrade","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting https://github.com/sobhe/hazm/archive/master.zip\n  Downloading https://github.com/sobhe/hazm/archive/master.zip\n\u001b[K     - 839 kB 445 kB/s\n\u001b[?25hCollecting libwapiti>=0.2.1\n  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n\u001b[K     |████████████████████████████████| 233 kB 1.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\nCollecting nltk==3.4\n  Downloading nltk-3.4.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\nRequirement already satisfied: singledispatch in /opt/conda/lib/python3.7/site-packages (from nltk==3.4->hazm==0.7.1) (3.4.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\nBuilding wheels for collected packages: hazm, libwapiti, nltk\n  Building wheel for hazm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hazm: filename=hazm-0.7.1-py3-none-any.whl size=318790 sha256=21209e208259be9f32819a1080783eb58cc0ba4571761d2c960e68fb580d281f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-jbw072gg/wheels/8f/8d/c6/ab0dcaf889f1a755e0ec972a802753c20f160b41c03472b7b3\n  Building wheel for libwapiti (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=169423 sha256=5c3ade5371e5e97763aea345767e739f3c1a1fece8f131f6df8af508f4e93cd6\n  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436381 sha256=7c0abb9fd4c60df549ebcaa9185c493766d2b45d5f1832963a7fe6da43fab2c2\n  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\nSuccessfully built hazm libwapiti nltk\nInstalling collected packages: nltk, libwapiti, hazm\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.4 which is incompatible.\u001b[0m\nSuccessfully installed hazm-0.7.1 libwapiti-0.2.1 nltk-3.4\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab as pl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom collections import Counter\nfrom hazm import *\n\n\n\n# sklearn package\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\n#Packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder,KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\nfrom io import StringIO\nimport matplotlib.pyplot as plt\nimport requests\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom numpy import nan\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom collections import Counter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom numpy import array\nimport string\n\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndfs = pd.read_csv(\"../input/traindataset/train.csv\")\ndf = pd.DataFrame(data=np.asarray(dfs), columns=[\"#\", \"Text\",\"Category\"])\n\n\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head(n=1).to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndata= np.asarray(df)\n# Use Hazm for preprocess of data :\nstopwords  = stopwords_list()\nnormalizer = Normalizer(persian_numbers=False)\npuncList   = [\".\", \";\", \":\", \"!\", \"?\", \"/\", \"\\\\\", \",\", \"#\", \"@\", \"$\", \"&\", \")\", \"(\", \"\\\"\", \"\\'\\'\"]\nstopwords.append(puncList)\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in range(len(data)):\n    data[i][1] = normalizer.normalize(data[i][1])\n    \n    data[i][1]=re.sub('[0-9]+','',data[i][1])\n    data[i][1]=re.sub('\\n','',data[i][1])\n    data[i][1]=re.sub('[^\\w\\s]','',data[i][1])\n    data[i][1]=re.sub('[A-Za-z]','',data[i][1])\n    data[i][1]=re.sub('\\_','',data[i][1])\n    data[i][1]=re.sub('[\\u200C\\u200F]+','',data[i][1])\n    data[i][1]= ' '.join(data[i][1].split())\n    #data[i][1] = data[i][1].split(' ') \n    #data[i][1] =' '.join( [w for w in  data[i][1] if not w in stopwords])\n    #data[i][1] = [w for w in  data[i][1] if not w in stopwords_]\n    \n\n#print(data)\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train   = data[ : ,1].tolist()\ny_train   = data[ : ,2].tolist()\n#print(y_train)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_train       = label_encoder.fit_transform(y_train)\n#print(y_train[0:20])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfsTest = pd.read_csv(\"../input/testdataset/test.csv\")\ndfTest = pd.DataFrame(data=np.asarray(dfsTest), columns=[\"#\", \"Text\"])\n#print(dfTest)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTest=np.asarray(dfTest)\nfor i in range(len(dataTest)):\n    dataTest[i][1] = normalizer.normalize(dataTest[i][1])\n    \n    dataTest[i][1]=re.sub('[0-9]+','',dataTest[i][1])\n    dataTest[i][1]=re.sub('[^\\w\\s]','',dataTest[i][1])\n    dataTest[i][1]=re.sub('[A-Za-z]','',dataTest[i][1])\n    dataTest[i][1]=re.sub('\\_','',dataTest[i][1])\n    dataTest[i][1]=re.sub('[\\u200C\\u200F]+','',dataTest[i][1])\n    dataTest[i][1]= ' '.join(dataTest[i][1].split())\n    \n\n#print(dataTest)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test   = dataTest[ : ,1].tolist()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    ngram_range=(1, 2)\n    )","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=word_vectorizer.fit_transform(X_train)\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nLSVC = LinearSVC()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSVC.fit(X_train,y_train)\ny2_LSVC_model = LSVC.predict(X_train)\nprint(\"LSVC Accuracy :\", accuracy_score(y_train, y2_LSVC_model))","execution_count":15,"outputs":[{"output_type":"stream","text":"LSVC Accuracy : 0.9992737981025477\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nX_test=word_vectorizer.transform(X_test)\n\npredictions_ = LSVC.predict(X_test)\nprint(predictions_[1:20])\npredictions__test=label_encoder.inverse_transform(predictions_)\nlist=[i for i in range(len(predictions__test))]\n\nDF = pd.DataFrame({\"Id\" : list, \"Category\" : predictions__test})\n\nDF.to_csv(\"Submission23.csv\", index=False)","execution_count":16,"outputs":[{"output_type":"stream","text":"[13  0 23 21 21  0 18 18 18 21 14 28 19 28 31 13 21  7 23]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"تغییر پارامترهای vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train   = data[ : ,1].tolist()\ny_train   = data[ : ,2].tolist()\nlabel_encoder = LabelEncoder()\ny_train       = label_encoder.fit_transform(y_train)\nX_test   = dataTest[ : ,1].tolist()\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    ngram_range=(1, 2),min_df=5\n    )\nX_train=word_vectorizer.fit_transform(X_train)\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSVC.fit(X_train,y_train)\ny2_LSVC_model = LSVC.predict(X_train)\nprint(\"LSVC Accuracy :\", accuracy_score(y_train, y2_LSVC_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nX_test=word_vectorizer.transform(X_test)\n\npredictions_ = LSVC.predict(X_test)\nprint(predictions_[1:20])\npredictions__test=label_encoder.inverse_transform(predictions_)\nlist=[i for i in range(len(predictions__test))]\n\nDF = pd.DataFrame({\"Id\" : list, \"Category\" : predictions__test})\n\nDF.to_csv(\"Submission230.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}