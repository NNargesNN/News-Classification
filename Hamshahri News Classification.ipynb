{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ghaseminarges/project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/sobhe/hazm/archive/master.zip\n",
      "  Downloading https://github.com/sobhe/hazm/archive/master.zip\n",
      "\u001b[K     - 839 kB 445 kB/s\n",
      "\u001b[?25hCollecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\n",
      "Collecting nltk==3.4\n",
      "  Downloading nltk-3.4.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\n",
      "Requirement already satisfied: singledispatch in /opt/conda/lib/python3.7/site-packages (from nltk==3.4->hazm==0.7.1) (3.4.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from libwapiti>=0.2.1->hazm==0.7.1) (1.14.0)\n",
      "Building wheels for collected packages: hazm, libwapiti, nltk\n",
      "  Building wheel for hazm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hazm: filename=hazm-0.7.1-py3-none-any.whl size=318790 sha256=21209e208259be9f32819a1080783eb58cc0ba4571761d2c960e68fb580d281f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jbw072gg/wheels/8f/8d/c6/ab0dcaf889f1a755e0ec972a802753c20f160b41c03472b7b3\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=169423 sha256=5c3ade5371e5e97763aea345767e739f3c1a1fece8f131f6df8af508f4e93cd6\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436381 sha256=7c0abb9fd4c60df549ebcaa9185c493766d2b45d5f1832963a7fe6da43fab2c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n",
      "Successfully built hazm libwapiti nltk\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.4 which is incompatible.\u001b[0m\n",
      "Successfully installed hazm-0.7.1 libwapiti-0.2.1 nltk-3.4\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/sobhe/hazm/archive/master.zip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from hazm import *\n",
    "\n",
    "\n",
    "\n",
    "# sklearn package\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "#Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from numpy import nan\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import array\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = pd.read_csv(\"../input/traindataset/train.csv\")\n",
    "df = pd.DataFrame(data=np.asarray(dfs), columns=[\"#\", \"Text\",\"Category\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(n=1).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data= np.asarray(df)\n",
    "# Use Hazm for preprocess of data :\n",
    "stopwords  = stopwords_list()\n",
    "normalizer = Normalizer(persian_numbers=False)\n",
    "puncList   = [\".\", \";\", \":\", \"!\", \"?\", \"/\", \"\\\\\", \",\", \"#\", \"@\", \"$\", \"&\", \")\", \"(\", \"\\\"\", \"\\'\\'\"]\n",
    "stopwords.append(puncList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(data)):\n",
    "    data[i][1] = normalizer.normalize(data[i][1])\n",
    "    \n",
    "    data[i][1]=re.sub('[0-9]+','',data[i][1])\n",
    "    data[i][1]=re.sub('\\n','',data[i][1])\n",
    "    data[i][1]=re.sub('[^\\w\\s]','',data[i][1])\n",
    "    data[i][1]=re.sub('[A-Za-z]','',data[i][1])\n",
    "    data[i][1]=re.sub('\\_','',data[i][1])\n",
    "    data[i][1]=re.sub('[\\u200C\\u200F]+','',data[i][1])\n",
    "    data[i][1]= ' '.join(data[i][1].split())\n",
    "    #data[i][1] = data[i][1].split(' ') \n",
    "    #data[i][1] =' '.join( [w for w in  data[i][1] if not w in stopwords])\n",
    "    #data[i][1] = [w for w in  data[i][1] if not w in stopwords_]\n",
    "    \n",
    "\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train   = data[ : ,1].tolist()\n",
    "y_train   = data[ : ,2].tolist()\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train       = label_encoder.fit_transform(y_train)\n",
    "#print(y_train[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsTest = pd.read_csv(\"../input/testdataset/test.csv\")\n",
    "dfTest = pd.DataFrame(data=np.asarray(dfsTest), columns=[\"#\", \"Text\"])\n",
    "#print(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest=np.asarray(dfTest)\n",
    "for i in range(len(dataTest)):\n",
    "    dataTest[i][1] = normalizer.normalize(dataTest[i][1])\n",
    "    \n",
    "    dataTest[i][1]=re.sub('[0-9]+','',dataTest[i][1])\n",
    "    dataTest[i][1]=re.sub('[^\\w\\s]','',dataTest[i][1])\n",
    "    dataTest[i][1]=re.sub('[A-Za-z]','',dataTest[i][1])\n",
    "    dataTest[i][1]=re.sub('\\_','',dataTest[i][1])\n",
    "    dataTest[i][1]=re.sub('[\\u200C\\u200F]+','',dataTest[i][1])\n",
    "    dataTest[i][1]= ' '.join(dataTest[i][1].split())\n",
    "    \n",
    "\n",
    "#print(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test   = dataTest[ : ,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=word_vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "LSVC = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSVC Accuracy : 0.9992737981025477\n"
     ]
    }
   ],
   "source": [
    "LSVC.fit(X_train,y_train)\n",
    "y2_LSVC_model = LSVC.predict(X_train)\n",
    "print(\"LSVC Accuracy :\", accuracy_score(y_train, y2_LSVC_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  0 23 21 21  0 18 18 18 21 14 28 19 28 31 13 21  7 23]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_test=word_vectorizer.transform(X_test)\n",
    "\n",
    "predictions_ = LSVC.predict(X_test)\n",
    "print(predictions_[1:20])\n",
    "predictions__test=label_encoder.inverse_transform(predictions_)\n",
    "list=[i for i in range(len(predictions__test))]\n",
    "\n",
    "DF = pd.DataFrame({\"Id\" : list, \"Category\" : predictions__test})\n",
    "\n",
    "DF.to_csv(\"Submission23.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
